{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a process of breaking the input sequence into smaller parts so that we can some useful smaller units of sequence for semantic processing. Tokens can be a word, sentence, paragraph etc. But in general, mostly tokens are considered words in real life applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are so many tokenizers available in `nltk` library, we will see below three tokenizers and how they work - \n",
    "<ol>\n",
    "    <li><b>WhitespaceTokenizer:</b>It tokenizes the string on whitespace</li>\n",
    "    <li><b>WordPunctTokenizer:</b>It tokenizes a text into a sequence of alphabetic and non-alphabetic characters, using the regexp ``\\w+|[^\\w\\s]+``.</li>\n",
    "    <li><b>TreebankWordTokenizer:</b>The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Today is Rahul's first class, isn't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Today'], ['is'], [\"Rahul's\"], ['first'], ['class,'], [\"isn't\"], ['it?']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wst = WhitespaceTokenizer()\n",
    "[wst.tokenize(token) for token in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Disadvantage:</b> it can't distinguish between `it` and `it?` which are different tokens with same meaning</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Today'],\n",
       " ['is'],\n",
       " ['Rahul', \"'\", 's'],\n",
       " ['first'],\n",
       " ['class', ','],\n",
       " ['isn', \"'\", 't'],\n",
       " ['it', '?']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "wpt = WordPunctTokenizer()\n",
    "[wpt.tokenize(token) for token in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Disadvantage:</b> it gives some tokes which are not meaningful such as - `s`, `isn`, `t`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Today'],\n",
       " ['is'],\n",
       " ['Rahul', \"'s\"],\n",
       " ['first'],\n",
       " ['class', ','],\n",
       " ['is', \"n't\"],\n",
       " ['it', '?']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tbt = TreebankWordTokenizer()\n",
    "[tbt.tokenize(token) for token in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Advantage:</b> it is better than the above two tokenizers, here we can see `'s` and `n't` are more meaningful for processing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is text normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while doing text analysis we may want to convert different varieties of words having similar meaning into a single entity. For example - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>This `computer` is taking too much `computation` time while `computing` on this data set<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example - are the words `computer`, `computation` and `computing` have different meaning? But computer will treat them differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>computer, computation, computing -> compute</li>\n",
    "    <li>run, runs, running -> run</li>\n",
    "</ol>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of text normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Stemming:</b> It is a process of removing and replacing suffixes to get to the root form of the word which is known as `stem`. There are three popular English stemmer - \n",
    "<ol>\n",
    "    <li>Porter</li>\n",
    "    <li>Lancester</li>\n",
    "</ol>    \n",
    "Stemming is generally a set of heuristics that chopps off suffixes. Porter stemmer has `5 heuristic phases` of word reductions that are applied sequentially. It is often noticed that stemming algorithm does not produce a real word after removing the stem, which is fine. But the purpose of stemming is to bring variant forms of a word together, not to map a word onto its ‘paradigm’ form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1a\n",
    "\n",
    "    SSES -> SS                         caresses  ->  caress\n",
    "    IES  -> I                          ponies    ->  poni\n",
    "                                       ties      ->  ti\n",
    "    SS   -> SS                         caress    ->  caress\n",
    "    S    ->                            cats      ->  cat\n",
    "\n",
    "Step 1b\n",
    "\n",
    "    (m>0) EED -> EE                    feed      ->  feed\n",
    "                                       agreed    ->  agree\n",
    "    (*v*) ED  ->                       plastered ->  plaster\n",
    "                                       bled      ->  bled\n",
    "    (*v*) ING ->                       motoring  ->  motor\n",
    "                                       sing      ->  sing\n",
    "\n",
    "If the second or third of the rules in Step 1b is successful, the following\n",
    "is done:\n",
    "\n",
    "    AT -> ATE                       conflat(ed)  ->  conflate\n",
    "    BL -> BLE                       troubl(ed)   ->  trouble\n",
    "    IZ -> IZE                       siz(ed)      ->  size\n",
    "    (*d and not (*L or *S or *Z))\n",
    "       -> single letter\n",
    "                                    hopp(ing)    ->  hop\n",
    "                                    tann(ed)     ->  tan\n",
    "                                    fall(ing)    ->  fall\n",
    "                                    hiss(ing)    ->  hiss\n",
    "                                    fizz(ed)     ->  fizz\n",
    "    (m=1 and *o) -> E               fail(ing)    ->  fail\n",
    "                                    fil(ing)     ->  file\n",
    "\n",
    "The rule to map to a single letter causes the removal of one of the double\n",
    "letter pair. The -E is put back on -AT, -BL and -IZ, so that the suffixes\n",
    "-ATE, -BLE and -IZE can be recognised later. This E may be removed in step\n",
    "4.\n",
    "\n",
    "Step 1c\n",
    "\n",
    "    (*v*) Y -> I                    happy        ->  happi\n",
    "                                    sky          ->  sky\n",
    "\n",
    "Step 1 deals with plurals and past participles. The subsequent steps are\n",
    "much more straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>You can go through the remaining rules of Porter stemming algorithm using this [link](https://tartarus.org/martin/PorterStemmer/def.txt)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer.stem('Germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thi comput is take too much comput time while comput on thi data set'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"This computer is taking too much computation time while computing on this data set\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\" \".join([stemmer.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Disadvantage:</b> It fails on irregular forms and thereby produces non-words. In the above you can see that it has produced non-words like `thi` and `comput`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Lemmatization:</b> It is a process of converting different versions of a word with the use of vocabulary and morphological analysis. It returns the base or a dictionary form of a word which is known as `lemma` - \n",
    "<ol>\n",
    "    <li>WordNet Lemmatizer</li>\n",
    "</ol>    \n",
    "WordNet lemmatization uses `WordNet` database to look up the lemmas. You can check the database using this [link](http://wordnetweb.princeton.edu/perl/webwn), and this database is also downloaded when we use `nltk` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The wolf are at the door'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The wolves are at the door\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\" \".join([lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer.lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the presentation could have been better'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"the presentation could have been better\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\" \".join([lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he talked to the person'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"he talked to the person\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\" \".join([lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Disadvantage:</b> Not all forms are reduced. As you can see above `talked` remains `talked`, no changes there. But let's see what happens if we stemmer in place of lemmatizer in this sentence</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he talk to the person'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"he talked to the person\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\" \".join([stemmer.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Did you noticed the difference?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Q1. So when to use stemming and when to use lemmatization?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Let's see the below examples - \n",
    "<ol>\n",
    "    <li>Us, us -> Are they same?</li>\n",
    "    <li>us, US -> Are they same?</li>\n",
    "</ol></font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>To solve these kinds of problems - \n",
    "We can \n",
    "<ol>\n",
    "    <li>lowercase beginning of the sentence</li>\n",
    "    <li>lowercasing words in titles</li>\n",
    "    <li>leave mid-sentence words as-is</li>\n",
    "<ol></font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Stopwords are English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# List stop words\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definitely', 'watch', 'part', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human', 'people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "text = \"The first time - you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2.\\\n",
    "It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\"\n",
    "\n",
    "# normalizing the text\n",
    "import re\n",
    "text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "\n",
    "# tokenizing it\n",
    "words = text.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'boring', 'look', 'least', 'twice', 'definitely', 'watch', 'part', 'change', 'view', 'matrix', 'human', 'people', 'ones', 'started', 'war', 'ai', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# removing stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first time see second renaissance may look boring look least twice definitely watch part change view matrix human people ones started war ai bad thing\n"
     ]
    }
   ],
   "source": [
    "# complete sentence after removing stop words\n",
    "sentence = \" \".join([w for w in words if w not in stopwords.words(\"english\")])\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('thank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('thanks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
